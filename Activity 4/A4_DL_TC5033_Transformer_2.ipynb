{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41b7905f-a070-4ffe-abfc-67fbcd2adaa9",
      "metadata": {
        "id": "41b7905f-a070-4ffe-abfc-67fbcd2adaa9"
      },
      "source": [
        "## TC 5033\n",
        "## Deep Learning\n",
        "## Transformers\n",
        "\n",
        "#### Activity 4: Implementing a Translator\n",
        "\n",
        "- Objective\n",
        "\n",
        "To understand the Transformer Architecture by Implementing a translator.\n",
        "\n",
        "- Instructions\n",
        "\n",
        "    This activity requires submission in teams. While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
        "\n",
        "    Follow the provided code. The code already implements a transformer from scratch as explained in one of [week's 9 videos](https://youtu.be/XefFj4rLHgU)\n",
        "\n",
        "    Since the provided code already implements a simple translator, your job for this assignment is to understand it fully, and document it using pictures, figures, and markdown cells.  You should test your translator with at least 10 sentences. The dataset used for this task was obtained from [Tatoeba, a large dataset of sentences and translations](https://tatoeba.org/en/downloads).\n",
        "  \n",
        "- Evaluation Criteria\n",
        "\n",
        "    - Code Readability and Comments\n",
        "    - Traning a translator\n",
        "    - Translating at least 10 sentences.\n",
        "\n",
        "- Submission\n",
        "\n",
        "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f240f0d8-d9e0-4632-962f-1a5a7881cb5f",
      "metadata": {
        "id": "f240f0d8-d9e0-4632-962f-1a5a7881cb5f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "QwKAge62suwG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwKAge62suwG",
        "outputId": "22478729-434b-42a0-90ac-d3a85e1c5f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17f54c65",
      "metadata": {
        "id": "17f54c65"
      },
      "source": [
        "#### Script to convert csv to text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8f02c0c2",
      "metadata": {
        "id": "8f02c0c2"
      },
      "outputs": [],
      "source": [
        "#This script requires to convert the TSV file to CSV\n",
        "# easiest way is to open it in Calc or excel and save as csv\n",
        "PATH = '/content/drive/MyDrive/AMLM/eng-spa/eng-spa-2024.csv'\n",
        "import pandas as pd\n",
        "df = pd.read_csv(PATH, sep=',', encoding='unicode_escape')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "787d9408",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "787d9408",
        "outputId": "e0c24d38-e7e8-4e69-a3e0-3b95c98428f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-dcb2613bd4ce>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  eng_spa_cols['length'] = eng_spa_cols.iloc[:, 0].str.len()\n"
          ]
        }
      ],
      "source": [
        "eng_spa_cols = df.iloc[:, [1, 3]]\n",
        "eng_spa_cols['length'] = eng_spa_cols.iloc[:, 0].str.len()\n",
        "eng_spa_cols = eng_spa_cols.sort_values(by='length')\n",
        "eng_spa_cols = eng_spa_cols.drop(columns=['length'])\n",
        "\n",
        "output_file_path = '/content/drive/MyDrive/AMLM/eng-spa/eng-spa4.txt'\n",
        "eng_spa_cols.to_csv(output_file_path, sep='\\t', index=False, header=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d468e9a",
      "metadata": {
        "id": "7d468e9a"
      },
      "source": [
        "## Transformer - Attention is all you need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d5dcf681",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5dcf681",
        "outputId": "c544e89c-0042-4317-a9aa-9b967c358146"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7dbe145f4410>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "torch.manual_seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2c2cbd17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c2cbd17",
        "outputId": "ba272554-397c-49ee-d7fa-7c421d69bc50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9c6623a1",
      "metadata": {
        "id": "9c6623a1"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LEN = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3103d45f",
      "metadata": {
        "id": "3103d45f"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n",
        "        super().__init__()\n",
        "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
        "        token_pos = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float()\n",
        "                             * (-math.log(10000.0)/d_model))\n",
        "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
        "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
        "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "#         print(self.pos_embed_matrix.shape)\n",
        "#         print(x.shape)\n",
        "        return x + self.pos_embed_matrix[:x.size(0), :]\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model = 512, num_heads = 8):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n",
        "\n",
        "        self.d_v = d_model // num_heads\n",
        "        self.d_k = self.d_v\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask = None):\n",
        "        batch_size = Q.size(0)\n",
        "        '''\n",
        "        Q, K, V -> [batch_size, seq_len, num_heads*d_k]\n",
        "        after transpose Q, K, V -> [batch_size, num_heads, seq_len, d_k]\n",
        "        '''\n",
        "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
        "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
        "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
        "\n",
        "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
        "        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads*self.d_k)\n",
        "        weighted_values = self.W_o(weighted_values)\n",
        "\n",
        "        return weighted_values, attention\n",
        "\n",
        "\n",
        "    def scale_dot_product(self, Q, K, V, mask = None):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attention = F.softmax(scores, dim = -1)\n",
        "        weighted_values = torch.matmul(attention, V)\n",
        "\n",
        "        return weighted_values, attention\n",
        "\n",
        "\n",
        "class PositionFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(F.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderSubLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.droupout1 = nn.Dropout(dropout)\n",
        "        self.droupout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        attention_score, _ = self.self_attn(x, x, x, mask)\n",
        "        x = x + self.droupout1(attention_score)\n",
        "        x = self.norm1(x)\n",
        "        x = x + self.droupout2(self.ffn(x))\n",
        "        return self.norm2(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderSubLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, target_mask=None, encoder_mask=None):\n",
        "        attention_score, _ = self.self_attn(x, x, x, target_mask)\n",
        "        x = x + self.dropout1(attention_score)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        encoder_attn, _ = self.cross_attn(x, encoder_output, encoder_output, encoder_mask)\n",
        "        x = x + self.dropout2(encoder_attn)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout3(ff_output)\n",
        "        return self.norm3(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "61070162",
      "metadata": {
        "id": "61070162"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, num_layers,\n",
        "                 input_vocab_size, target_vocab_size,\n",
        "                 max_len=MAX_SEQ_LEN, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
        "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
        "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
        "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        # Encoder mask\n",
        "        source_mask, target_mask = self.mask(source, target)\n",
        "        # Embedding and positional Encoding\n",
        "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim)\n",
        "        source = self.pos_embedding(source)\n",
        "        # Encoder\n",
        "        encoder_output = self.encoder(source, source_mask)\n",
        "\n",
        "        # Decoder embedding and postional encoding\n",
        "        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n",
        "        target = self.pos_embedding(target)\n",
        "        # Decoder\n",
        "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
        "\n",
        "        return self.output_layer(output)\n",
        "\n",
        "\n",
        "\n",
        "    def mask(self, source, target):\n",
        "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
        "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
        "        size = target.size(1)\n",
        "        no_mask = torch.tril(torch.ones((1, size, size), device=device)).bool()\n",
        "        target_mask = target_mask & no_mask\n",
        "        return source_mask, target_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6da6b2d4",
      "metadata": {
        "id": "6da6b2d4"
      },
      "source": [
        "#### Simple test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d40581d6",
      "metadata": {
        "id": "d40581d6"
      },
      "outputs": [],
      "source": [
        "seq_len_source = 10\n",
        "seq_len_target = 10\n",
        "batch_size = 2\n",
        "input_vocab_size = 50\n",
        "target_vocab_size = 50\n",
        "\n",
        "source = torch.randint(1, input_vocab_size, (batch_size, seq_len_source))\n",
        "target = torch.randint(1, target_vocab_size, (batch_size, seq_len_target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fc7cf689",
      "metadata": {
        "id": "fc7cf689"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "num_layers = 6\n",
        "\n",
        "model = Transformer(d_model, num_heads, d_ff, num_layers,\n",
        "                  input_vocab_size, target_vocab_size,\n",
        "                  max_len=MAX_SEQ_LEN, dropout=0.1)\n",
        "\n",
        "model = model.to(device)\n",
        "source = source.to(device)\n",
        "target = target.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4618560e",
      "metadata": {
        "id": "4618560e"
      },
      "outputs": [],
      "source": [
        "output = model(source, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ab0bc69d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab0bc69d",
        "outputId": "8678bc58-4aa9-4306-84fa-ac2581a25baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ouput.shape torch.Size([2, 10, 50])\n"
          ]
        }
      ],
      "source": [
        "# Expected output shape -> [batch, seq_len_target, target_vocab_size] i.e. [2, 10, 50]\n",
        "print(f'ouput.shape {output.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f4b2910",
      "metadata": {
        "id": "0f4b2910"
      },
      "source": [
        "### Translator Eng-Spa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "869a7244",
      "metadata": {
        "id": "869a7244"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/MyDrive/AMLM/eng-spa/eng-spa4.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d0af1eba",
      "metadata": {
        "id": "d0af1eba"
      },
      "outputs": [],
      "source": [
        "with open(PATH, 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "eng_spa_pairs = [line.strip().split('\\t') for line in lines if '\\t' in line]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c930226f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c930226f",
        "outputId": "e10e0968-5306-475c-e1a4-47eea1ddb4b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Go.', 'Váyase.'],\n",
              " ['Ah!', '¡Anda!'],\n",
              " ['Ok!', '¡OK!'],\n",
              " ['No.', 'No.'],\n",
              " ['So?', '¿Y qué?'],\n",
              " ['OK.', '¡Órale!'],\n",
              " ['Go!', '¡Sal!'],\n",
              " ['Go!', '¡Ve!'],\n",
              " ['Ow!', '¡Ay!'],\n",
              " ['Go.', 'Vaya.']]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "eng_spa_pairs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "095f4037",
      "metadata": {
        "id": "095f4037"
      },
      "outputs": [],
      "source": [
        "eng_sentences = [pair[0] for pair in eng_spa_pairs]\n",
        "spa_sentences = [pair[1] for pair in eng_spa_pairs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0d9e1c95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d9e1c95",
        "outputId": "0b25260c-1c10-4d18-eb0e-4c3b92093173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Go.', 'Ah!', 'Ok!', 'No.', 'So?', 'OK.', 'Go!', 'Go!', 'Ow!', 'Go.']\n",
            "['Váyase.', '¡Anda!', '¡OK!', 'No.', '¿Y qué?', '¡Órale!', '¡Sal!', '¡Ve!', '¡Ay!', 'Vaya.']\n"
          ]
        }
      ],
      "source": [
        "print(eng_sentences[:10])\n",
        "print(spa_sentences[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "60d11478",
      "metadata": {
        "id": "60d11478"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[á]+\", \"a\", sentence)\n",
        "    sentence = re.sub(r\"[é]+\", \"e\", sentence)\n",
        "    sentence = re.sub(r\"[í]+\", \"i\", sentence)\n",
        "    sentence = re.sub(r\"[ó]+\", \"o\", sentence)\n",
        "    sentence = re.sub(r\"[ú]+\", \"u\", sentence)\n",
        "    sentence = re.sub(r\"[^a-z]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    sentence = '<sos> ' + sentence + ' <eos>'\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "478f673b",
      "metadata": {
        "id": "478f673b"
      },
      "outputs": [],
      "source": [
        "s1 = '¿Hola @ cómo estás? 123'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "96ac79c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96ac79c5",
        "outputId": "de08836f-0848-4785-f314-893c3f4d5a92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¿Hola @ cómo estás? 123\n",
            "<sos> hola como estas <eos>\n"
          ]
        }
      ],
      "source": [
        "print(s1)\n",
        "print(preprocess_sentence(s1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d9fc9c4d",
      "metadata": {
        "id": "d9fc9c4d"
      },
      "outputs": [],
      "source": [
        "eng_sentences = [preprocess_sentence(sentence) for sentence in eng_sentences]\n",
        "spa_sentences = [preprocess_sentence(sentence) for sentence in spa_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f7a3b18d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7a3b18d",
        "outputId": "53179d2d-641c-4700-a3b5-47d22c7143e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos> vayase <eos>',\n",
              " '<sos> anda <eos>',\n",
              " '<sos> ok <eos>',\n",
              " '<sos> no <eos>',\n",
              " '<sos> y que <eos>',\n",
              " '<sos> orale <eos>',\n",
              " '<sos> sal <eos>',\n",
              " '<sos> ve <eos>',\n",
              " '<sos> ay <eos>',\n",
              " '<sos> vaya <eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "spa_sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "97931cd3",
      "metadata": {
        "id": "97931cd3"
      },
      "outputs": [],
      "source": [
        "def build_vocab(sentences):\n",
        "    words = [word for sentence in sentences for word in sentence.split()]\n",
        "    word_count = Counter(words)\n",
        "    sorted_word_counts = sorted(word_count.items(), key=lambda x:x[1], reverse=True)\n",
        "    word2idx = {word: idx for idx, (word, _) in enumerate(sorted_word_counts, 2)}\n",
        "    word2idx['<pad>'] = 0\n",
        "    word2idx['<unk>'] = 1\n",
        "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "    return word2idx, idx2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7fa8738e",
      "metadata": {
        "id": "7fa8738e"
      },
      "outputs": [],
      "source": [
        "eng_word2idx, eng_idx2word = build_vocab(eng_sentences)\n",
        "spa_word2idx, spa_idx2word = build_vocab(spa_sentences)\n",
        "eng_vocab_size = len(eng_word2idx)\n",
        "spa_vocab_size = len(spa_word2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "79d6b633",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79d6b633",
        "outputId": "9a4c2f8e-68a3-4034-a981-dba0be86034d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27650 46924\n"
          ]
        }
      ],
      "source": [
        "print(eng_vocab_size, spa_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e564017c",
      "metadata": {
        "id": "e564017c"
      },
      "outputs": [],
      "source": [
        "class EngSpaDataset(Dataset):\n",
        "    def __init__(self, eng_sentences, spa_sentences, eng_word2idx, spa_word2idx):\n",
        "        self.eng_sentences = eng_sentences\n",
        "        self.spa_sentences = spa_sentences\n",
        "        self.eng_word2idx = eng_word2idx\n",
        "        self.spa_word2idx = spa_word2idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.eng_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        eng_sentence = self.eng_sentences[idx]\n",
        "        spa_sentence = self.spa_sentences[idx]\n",
        "        # return tokens idxs\n",
        "        eng_idxs = [self.eng_word2idx.get(word, self.eng_word2idx['<unk>']) for word in eng_sentence.split()]\n",
        "        spa_idxs = [self.spa_word2idx.get(word, self.spa_word2idx['<unk>']) for word in spa_sentence.split()]\n",
        "\n",
        "        return torch.tensor(eng_idxs), torch.tensor(spa_idxs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b579577b",
      "metadata": {
        "id": "b579577b"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    eng_batch, spa_batch = zip(*batch)\n",
        "    eng_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in eng_batch]\n",
        "    spa_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in spa_batch]\n",
        "    eng_batch = torch.nn.utils.rnn.pad_sequence(eng_batch, batch_first=True, padding_value=0)\n",
        "    spa_batch = torch.nn.utils.rnn.pad_sequence(spa_batch, batch_first=True, padding_value=0)\n",
        "    return eng_batch, spa_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8d514b7c",
      "metadata": {
        "id": "8d514b7c"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, loss_function, optimiser, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i, (eng_batch, spa_batch) in enumerate(dataloader):\n",
        "            eng_batch = eng_batch.to(device)\n",
        "            spa_batch = spa_batch.to(device)\n",
        "            # Decoder preprocessing\n",
        "            target_input = spa_batch[:, :-1]\n",
        "            target_output = spa_batch[:, 1:].contiguous().view(-1)\n",
        "            # Zero grads\n",
        "            optimiser.zero_grad()\n",
        "            # run model\n",
        "            output = model(eng_batch, target_input)\n",
        "            output = output.view(-1, output.size(-1))\n",
        "            # loss\\\n",
        "            loss = loss_function(output, target_output)\n",
        "            # gradient and update parameters\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss/len(dataloader)\n",
        "        print(f'Epoch: {epoch}/{epochs}, Loss: {avg_loss:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "2379ea72",
      "metadata": {
        "id": "2379ea72"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e08eef6a",
      "metadata": {
        "id": "e08eef6a"
      },
      "outputs": [],
      "source": [
        "model = Transformer(d_model=512, num_heads=8, d_ff=2048, num_layers=6,\n",
        "                    input_vocab_size=eng_vocab_size, target_vocab_size=spa_vocab_size,\n",
        "                    max_len=MAX_SEQ_LEN, dropout=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "a1181a12",
      "metadata": {
        "id": "a1181a12"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.0001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "14e265e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14e265e9",
        "outputId": "c2668a18-40a0-4fbe-8eca-2105ee33922f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0/3, Loss: 3.5977\n",
            "Epoch: 1/3, Loss: 2.2018\n",
            "Epoch: 2/3, Loss: 1.7027\n"
          ]
        }
      ],
      "source": [
        "train(model, dataloader, loss_function, optimiser, epochs = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d271146",
      "metadata": {
        "id": "1d271146"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "50740746",
      "metadata": {
        "id": "50740746"
      },
      "outputs": [],
      "source": [
        "def sentence_to_indices(sentence, word2idx):\n",
        "    return [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n",
        "\n",
        "def indices_to_sentence(indices, idx2word):\n",
        "    return ' '.join([idx2word[idx] for idx in indices if idx in idx2word and idx2word[idx] != '<pad>'])\n",
        "\n",
        "def translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
        "    model.eval()\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    input_indices = sentence_to_indices(sentence, eng_word2idx)\n",
        "    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)\n",
        "\n",
        "    # Initialize the target tensor with <sos> token\n",
        "    tgt_indices = [spa_word2idx['<sos>']]\n",
        "    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            output = model(input_tensor, tgt_tensor)\n",
        "            output = output.squeeze(0)\n",
        "            next_token = output.argmax(dim=-1)[-1].item()\n",
        "            tgt_indices.append(next_token)\n",
        "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
        "            if next_token == spa_word2idx['<eos>']:\n",
        "                break\n",
        "\n",
        "    return indices_to_sentence(tgt_indices, spa_idx2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "c2c0db72",
      "metadata": {
        "id": "c2c0db72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0376a14b-f32a-4440-e013-be66d8414e45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence: Hello, how are you?\n",
            "Traducción: <sos> hola como estas <eos>\n",
            "\n",
            "Input sentence: I am learning artificial intelligence.\n",
            "Traducción: <sos> estoy aprendiendo inteligencia artificial <eos>\n",
            "\n",
            "Input sentence: Artificial intelligence is great.\n",
            "Traducción: <sos> la inteligencia artificial es grande <eos>\n",
            "\n",
            "Input sentence: Good night!\n",
            "Traducción: <sos> buenas noches <eos>\n",
            "\n",
            "Input sentence: I love to travel\n",
            "Traducción: <sos> me encanta viajar <eos>\n",
            "\n",
            "Input sentence: This lake provided water to our city.\n",
            "Traducción: <sos> nuestra ciudad se considera agua al lago <eos>\n",
            "\n",
            "Input sentence: We meet once a month.\n",
            "Traducción: <sos> nos reunimos una vez al mes <eos>\n",
            "\n",
            "Input sentence: I know how to do my job.\n",
            "Traducción: <sos> como se hacer mi trabajo <eos>\n",
            "\n",
            "Input sentence: Do you want to sit somewhere else?\n",
            "Traducción: <sos> quieres sentarse en otro lugar <eos>\n",
            "\n",
            "Input sentence: The purpose of life is a life of purpose.\n",
            "Traducción: <sos> la vida es el proposito de la vida <eos>\n",
            "\n",
            "Input sentence: She is not a good person.\n",
            "Traducción: <sos> no es una buena persona <eos>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def evaluate_translations(model, sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
        "    for sentence in sentences:\n",
        "        translation = translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len, device)\n",
        "        print(f'Input sentence: {sentence}')\n",
        "        print(f'Traducción: {translation}')\n",
        "        print()\n",
        "\n",
        "# Example sentences to test the translator\n",
        "test_sentences = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"I am learning artificial intelligence.\",\n",
        "    \"Artificial intelligence is great.\",\n",
        "    \"Good night!\",\n",
        "    \"I love to travel\",\n",
        "    \"This lake provided water to our city.\",\n",
        "    \"We meet once a month.\",\n",
        "    \"I know how to do my job.\",\n",
        "    \"Do you want to sit somewhere else?\",\n",
        "    \"The purpose of life is a life of purpose.\",\n",
        "    \"She is not a good person.\"\n",
        "]\n",
        "\n",
        "# Assuming the model is trained and loaded\n",
        "# Set the device to 'cpu' or 'cuda' as needed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Evaluate translations\n",
        "evaluate_translations(model, test_sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ceefe95",
      "metadata": {
        "id": "4ceefe95"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7e10a50",
      "metadata": {
        "id": "a7e10a50"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bffb7af9",
      "metadata": {
        "id": "bffb7af9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6321db74",
      "metadata": {
        "id": "6321db74"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccce7864",
      "metadata": {
        "id": "ccce7864"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}